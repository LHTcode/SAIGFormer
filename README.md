# SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement

## Abstract
Transformer-based end-to-end restoration frameworks have shown promising potential in low-light image enhancement (LLIE). However, the unified treatment of multiple degradation factors hinders the model from achieving optimal illumination restoration quality. Existing methods attempt to decouple illumination from low-light images; however, their illumination extraction strategies often either impose excessive constraints that hinder generalization or overlook the inherently non-uniform nature of real-world illumination. To address these issues, we propose SAIGFormer, a U-shaped network composed of stacked Spatially-Adaptive Illumination Guided Transformer (SAIGT) blocks. In SAIGFormer, we design a simple and fast module, the spatially-Adaptive Integral-image-based Illumination Extractor (AIÂ²E), to spatially adaptively extract illumination from the input low-light image. The extracted illumination is then used in the Spatially-Adaptive Illumination Guided Multi-head Self-Attention (SAIG-MSA) module to guide the Transformer in learning the illumination patterns. Our method is evaluated on multiple datasets, and achieves significant improvements over state-of-the-art (SOTA) methods across various quantitative and qualitative metrics.